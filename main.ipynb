{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script slugify.exe is installed in 'C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script kaggle.exe is installed in 'C:\\Users\\Usuario\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"kaggle\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"unzip\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download -d arushchillar/disneyland-reviews\n",
    "! unzip disneyland-reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./DisneylandReviews.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fines de generar un modelo de Sentiment Analysis, generaremos una variable Target con el sentimiento de cada review en base a la variable rating de la siguiente manera:\n",
    "1. Positive: Reseñas con puntaje 4 o 5\n",
    "2. Neutral: Reseñas con puntaje 3\n",
    "3. Negative: Reseñas con puntaje 1 y 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero columna target\n",
    "def Target(row):\n",
    "    sentiment = \"positive\"\n",
    "    if int(row[\"Rating\"]) == 3:\n",
    "        sentiment = \"neutral\"\n",
    "    elif int(row[\"Rating\"]) < 3:\n",
    "        sentiment = \"negative\"\n",
    "    return sentiment\n",
    "\n",
    "df[\"target\"] = df.apply(Target, axis=1)\n",
    "print(df.groupby(['target'])['target'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_counts = df['target'].value_counts()\n",
    "\n",
    "colors = {'positive': 'green', 'neutral': 'yellow', 'negative': 'red'}\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(target_counts, labels=target_counts.index, colors=[colors[sentiment] for sentiment in target_counts.index], autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')\n",
    "plt.title('Pie Chart of Sentiment for Disneyland')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Descriptivo\n",
    "Bag of Words Binarizado: asigna un 1 a todas las palabras del corpus que estén presentes en el documento\n",
    "\n",
    "Regresión Logística para evaluar qué palabras son las más significativas por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = df['Review_Text'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "matriz_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convierte la matriz BoW en un diccionario de BoW binarizado para cada documento\n",
    "diccionarios_bin = [dict(zip(vectorizer.get_feature_names_out(), row.toarray()[0])) for row in matriz_bow]\n",
    "print(diccionarios_bin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "rlog = LogisticRegression()\n",
    "X = diccionarios_bin\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "rlog.fit(X_train, y_train)\n",
    "\n",
    "pred = rlog.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "reporte_clasificacion = classification_report(y_test, pred)\n",
    "matriz_confusion = confusion_matrix(y_test, pred)\n",
    "\n",
    "print(f\"Precisión: {accuracy}\")\n",
    "print(reporte_clasificacion)\n",
    "print(matriz_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlog.fit(X, y)\n",
    "betas = rlog.coef_\n",
    "print(betas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words con todo el corpus: Devuelve frecuencia de aparición de cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install transformers nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('park', 44309), ('disney', 35973), ('rides', 34295), ('disneyland', 32622), ('time', 29219), ('day', 28145), ('get', 22963), ('go', 20091), ('one', 19081), ('ride', 17661), ('great', 16305), ('would', 14591), ('kids', 14135), ('food', 14129), ('place', 13367)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "diccionario = defaultdict(int)\n",
    "\n",
    "def get_word_freqs(corpus):\n",
    "    word_freqs = defaultdict(int)\n",
    "\n",
    "    for text in corpus:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        filtered_text = [word.lower() for word in text.split(\" \") if word.lower() not in stop_words]\n",
    "        filtered_text = \" \".join(filtered_text)\n",
    "        words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(filtered_text)\n",
    "        new_words = [word for word, _ in words_with_offsets]\n",
    "        for word in new_words:\n",
    "            word = word.lstrip(\"Ġ\")\n",
    "            if word != \"\":\n",
    "                word_freqs[word] += 1\n",
    "    \n",
    "    return word_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = get_word_freqs(corpus)\n",
    "print(sorted(diccionario.items(), key=lambda x: x[1], reverse=True)[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install wordcloud matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_word_cloud(corpus, filtered_words = []):\n",
    "    filtered_words.extend(stop_words)\n",
    "    text = \"\\n\".join(corpus)\n",
    "\n",
    "    wordcloud = WordCloud(stopwords=set(filtered_words)).generate(text)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "plot_word_cloud(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = [\"DISNEYLAND\", \"DISNEY\", \"RIDE\", \"RIDES\", \"PARK\", \"DAY\", \"GET\", \"ATTRACTION\", \"WENT\", \"GO\", \"ONE\", \"TIME\"]\n",
    "positive_corpus = df[df['target'] == 'positive']['Review_Text'].tolist()\n",
    "plot_word_cloud(positive_corpus, common_words)\n",
    "\n",
    "# neutral_corpus = df[df['target'] == 'neutral']['Review_Text'].tolist()\n",
    "# plot_word_cloud(neutral_corpus, common_words)\n",
    "\n",
    "negative_corpus = df[df['target'] == 'negative']['Review_Text'].tolist()\n",
    "plot_word_cloud(negative_corpus, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "branch_ratings_avg = df.groupby('Branch')['Rating'].mean()\n",
    "\n",
    "# Obtener las etiquetas de las sucursales\n",
    "branch_labels = branch_ratings_avg.index\n",
    "\n",
    "# Obtener los valores promedio de Rating\n",
    "ratings_avg_values = branch_ratings_avg.values\n",
    "\n",
    "# Crear un gráfico de barras\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.bar(branch_labels, ratings_avg_values, color='darkblue', alpha=0.7)\n",
    "plt.xlabel('Branch')\n",
    "plt.ylabel('Rating Promedio')\n",
    "plt.title('Promedio de Rating por Branch')\n",
    "plt.xticks(rotation=45)  # Rotar las etiquetas del eje x para una mejor visualización\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disneyParis = df[df['Branch'] == 'Disneyland_Paris']\n",
    "\n",
    "disneyParis[\"target\"] = df.apply(Target, axis=1)\n",
    "\n",
    "target_counts_paris = disneyParis['target'].value_counts()\n",
    "\n",
    "\n",
    "colors = ['green', 'yellow', 'red']\n",
    "\n",
    "# Crear el gráfico de torta\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(target_counts_paris, labels=target_counts_paris.index, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Para asegurarte de que el gráfico sea un círculo\n",
    "plt.title('Pie Chart of Sentiment for Disneyland_Paris')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisneyHongKong = df[df['Branch'] == 'Disneyland_HongKong']\n",
    "\n",
    "DisneyHongKong[\"target\"] = df.apply(Target, axis=1)\n",
    "\n",
    "target_counts_hongkong = DisneyHongKong['target'].value_counts()\n",
    "\n",
    "\n",
    "colors = ['green', 'yellow', 'red']\n",
    "\n",
    "# Crear el gráfico de torta\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(target_counts_hongkong, labels=target_counts_hongkong.index, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Para asegurarte de que el gráfico sea un círculo\n",
    "plt.title('Pie Chart of Sentiment for Disneyland_HongKong')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DisneyCalifornia = df[df['Branch'] == 'Disneyland_California']\n",
    "\n",
    "DisneyCalifornia[\"target\"] = df.apply(Target, axis=1)\n",
    "\n",
    "target_counts_california = DisneyCalifornia['target'].value_counts()\n",
    "\n",
    "\n",
    "colors = ['green', 'yellow', 'red']\n",
    "\n",
    "# Crear el gráfico de torta\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(target_counts_california, labels=target_counts_california.index, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Para asegurarte de que el gráfico sea un círculo\n",
    "plt.title('Pie Chart of Sentiment for California')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-Pair Encoding: Amplía el vocabulario con las agrupaciones de tokens más comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in vocab:\n",
    "            vocab.append(letter)\n",
    "vocab.sort()\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibe todos los caracteres separados de las palabras de los documentos, los empareja y calcula la frecuencia de los emparejamientos\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muestra el par más repetido\n",
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encuentra en las palabras los pares indicados y los agrupa\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "splits = merge_pair(\"i\", \"n\", splits)\n",
    "\n",
    "for i, key in enumerate(splits.keys()):\n",
    "    print(f\"{key}: {splits[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#muestra los pares más repetidos\n",
    "merges = {(\"i\", \"n\"): \"in\"}\n",
    "vocab.append(\"in\")\n",
    "\n",
    "vocab_size = 100 #setea el máximo de pares a insertar hasta que el vocab alcance dicho tamaño\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "\n",
    "print(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: Devuelve el \"peso\" de cada palabra dentro de cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genera peso para cada palabra de cada documento\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,1))\n",
    "bow = vect.fit_transform(df['Review_Text'])\n",
    "total_features = len(vect.vocabulary_)\n",
    "\n",
    "print(bow[0]) #muestro pesos de palabras del documento 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#muestro orden de palabras con mayor peso del documento 0\n",
    "print(sorted((bow[0,doc] for doc in range(total_features)), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow[0]==0.40736440254022876) #la palabra con el mayor peso del doc 0 es la 15187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.get_feature_names_out()[15187]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Review_Text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis Predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, df['target'], test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling con LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import words\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(X_train, num_topics=20, id2word=diccionario)\n",
    "\n",
    "train_topic_prob = lda_model.get_document_topics(X_train, minimum_probability=0.0) #devuelve probabilidad de pertenencia de cada documento a cada tópico\n",
    "most_probable_topic = [max(doc, key=lambda x: x[1])[0] for doc in train_topic_prob] #asigna a cada documento el tópico con mayor probabilidad\n",
    "\n",
    "valid_indices = corpus.index\n",
    "#------\n",
    "data = {'indice':valid_indices,'topico':most_probable_topic}\n",
    "indiceXtopico = pd.DataFrame(data)\n",
    "\n",
    "train['indice'] = train.index\n",
    "train = pd.merge(train, indiceXtopico, on='indice', how='left') #genero columna tópico en train\n",
    "train.drop('indice', axis=1, inplace=True)\n",
    "train.drop('tagline', axis=1, inplace=True)\n",
    "\n",
    "test_texts = val[val['tagline'].isna()==False]['tagline']\n",
    "processed_test_texts = [preprocess(doc) for doc in test_texts]\n",
    "\n",
    "#Coherencia de número de tópicos en test\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=processed_test_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "test_corpus = [dictionary.doc2bow(text) for text in processed_test_texts]\n",
    "\n",
    "test_topic_prob = lda_model.get_document_topics(test_corpus, minimum_probability=0.0) #devuelve probabilidad de pertenencia de cada documento a cada tópico\n",
    "most_probable_topic = [max(doc, key=lambda x: x[1])[0] for doc in test_topic_prob] #asigna a cada documento el tópico con mayor probabilidad\n",
    "\n",
    "valid_indices = val.index[~val['tagline'].isna()]\n",
    "data = {'indice':valid_indices,'topico':most_probable_topic}\n",
    "indiceXtopico = pd.DataFrame(data)\n",
    "\n",
    "val['indice'] = val.index\n",
    "val = pd.merge(val, indiceXtopico, on='indice', how='left') #genero columna tópico en val\n",
    "val.drop('indice', axis=1, inplace=True)\n",
    "val.drop('tagline', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo Preentrenado roBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "# config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "# model = RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "roberta = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(roberta, num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(roberta)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "for i in range(len(corpus[:10])):\n",
    "    tokens = tokenizer(corpus[i], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**tokens)\n",
    "    scores = outputs[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    max_score_index = scores.argmax()\n",
    "    \n",
    "    print(df['target'][i], labels[max_score_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create torch dataset\n",
    "import torch\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    \n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
